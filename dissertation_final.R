###############################
# Uber Movement Travel Times Data (unit: seconds)
###############################

### load packages
library(dplyr) # data manipulation '%>%', floating number comparison
library(stringr) # text extraction, function 'word'
library(jsonlite) # read json files
library(gdata) # read xlsx sheet data
library(varhandle) # handle factors to numeric values
require(ggplot2)
require(ggrepel) # text on ggplot
require(viridis) # ggplot2 continuous colour palette
library(rgdal) # read shapefile
library(broom) # use tidy function
library(fastDummies) # create dummy variables
## Logistic regression
library(MASS) # stepwise regression
library(tidyverse) # logistic regression - linear diagnostics
library(car) # VIF test (multicollinearity)
library(pscl) # psudo-R2
library(ResourceSelection) # Hosmer-Lemeshow Test for goodness of fit test
## XGBoost classifier
library(xgboost)
library(caret)    # for the confusionmatrix() function (also needs e1071 package)
library(Ckmeans.1d.dp) # for xgb.ggplot.importance
library(fmsb) # radar plot
## Time series clustering
library(dtwclust) # time series clustering
library(plyr) # round the numbers

### set working directory
setwd('./Documents/Manchester/Dissertation/Data(2016Q1_2019Q3)')

### read travel times data
# files exist under the folder 'Weekdays'
files <- list.files(path = paste(getwd(), "/Weekdays", sep = ""), pattern = "*.csv", full.names = T)
wd <- sapply(files, read.csv, simplify=FALSE) %>% bind_rows(.id = "time") # combine rows by 'time'

## reformat 'time': from file path to year-quarter format
# since year and quarter exist at the 3rd and 4th position separated by '-', reformat time by
# selecting words from these positions
wd$time <- c(sapply(wd$time, function(x) paste(word(x, 3:4, sep = fixed('-')), collapse = "")))
wd$time <- as.factor(wd$time) # set the correct type for 'time'


### read uber movement geo data
# file exists under folder 'Supplementary_Data'
geo <- fromJSON("Supplementary_Data/manchester_msoa.json", flatten = TRUE)$features
geo$properties.MOVEMENT_ID <- as.integer(geo$properties.MOVEMENT_ID) # set the same type for id as id from wd
# just keep the relevant information from geo data
s.geo <- geo[,c('properties.MOVEMENT_ID', 'properties.DISPLAY_NAME', 'properties.zone_id')]
rm(geo)


### read od (commute trips) data
## This data (sheet 'OD_All') has been compiled to present all the trips generated by road use.
od <- read.xls("./Supplementary_Data/City_Centrality/OD_Data(road).xlsx", sheet = 'OD_All', header = FALSE)
# row: residence; column: workplace

# Build City Centrality Factor ------------------------------------------------

# set the file path for city centrality factor
files_path <- './Supplementary_Data/City_Centrality'

##### residence centrality

### morphological: residence density
population <- read.csv(paste(files_path, '/Residents_Population/Data_Residents_Population.csv', sep = ''), header = TRUE, skip = 1)
colnames(population) <- c('id', 'geocode', 'zone_name', 'geo_layer', 'geo_layer_short', 'density', 'population')
population <- population %>% filter(zone_name %in% s.geo$properties.DISPLAY_NAME) # remove unused entries
residence_density_data <- population[, c('geocode', 'zone_name', 'density')]
rm(population)

#-----------------------

### functional: commute-back-home proportion. Low: place for work; High: place for residence (Zhong et al., 2017) 
# read employment data to get the total number of working individuals reside in each zone
emp_dist <- read.xls('./Supplementary_Data/socio-economic_characteristics_GM/Socio-economic_factors.xlsx',
                     sheet = 'Employment', skip = 1, header = TRUE)
colnames(emp_dist)[1:5] <- c('id', 'geocode', 'zone_name', 'layer', 'abbr_layer')

## build commute-back-home dataframe
od_zone <- t(od[1,]) # get the zone name from od data
od_flow <- od[3:nrow(od),] # get trips data
od_flow <- unfactor(od_flow) # unfactor to get the true number instead of level from factor data
residence_flow_data <- data.frame(zone_name = od_zone, residence_flow = rowSums(od_flow))
colnames(residence_flow_data)[1] <- 'zone_name'

# merge commute-back-home dataframe with employment
residence_flow_data <- merge(residence_flow_data, emp_dist[, c('zone_name', 'Total'),], by = 'zone_name')

# acquire commute-back-home proportion
residence_flow_data$backhome_prop <- residence_flow_data$residence_flow/residence_flow_data$Total # commute-back-home proportion

#-----------------------

### residence centre index
residence_centre_data <- merge(residence_density_data, residence_flow_data[, c('zone_name', 'backhome_prop')], by = 'zone_name')
cor(residence_centre_data[-c(1, 2)]) # r = -0.1933396, nearly no association between density and backhome_prop
# hence, we cannot combine these two variables into one index

##### employment centrality

### morphological: medium-large enterprise (employment size >= 50) ratio
enterprise_size <- read.csv(paste(files_path, '/enterprise_size.csv', sep = ''), skip = 8, header = TRUE)
colnames(enterprise_size) <- c('zone_name', 'geocode', 'micro_0-9', 'small_10-49', 'medium_50-249', 'large_250')

# reformat zone_name with only the name of zone without layer name by remove words before ':'
enterprise_size$zone_name <- sub('.*:', '', enterprise_size$zone_name)
enterprise_size <- enterprise_size %>% filter(enterprise_size$zone_name %in% s.geo$properties.DISPLAY_NAME) # remove unused entries

## calculate medium-large enterprise ratio of each zone
# total number of enterprise: sum up number of enterprise from all size range
enterprise_size$num_enterprise <- rowSums(enterprise_size[, 3:ncol(enterprise_size)])
# calculate the ratio of medium to large enterprise
enterprise_ratio <- rowSums(enterprise_size[, c('medium_50-249', 'large_250')])/enterprise_size$num_enterprise
enterprise_ratio_data <- data.frame(zone_name = enterprise_size$zone_name, enterprise_ratio = enterprise_ratio)

#-----------------------

# calculate commute-to-work flow to define employment centre
employment_flow_data <- data.frame(zone_name = od_zone, employment_flow = colSums(od_flow))
colnames(employment_flow_data)[1] <- 'zone_name'

#-----------------------

employment_centre_data <- merge(enterprise_ratio_data, employment_flow_data, by = 'zone_name')

### functional: economical activity diversity
emp_diversity <- read.csv(paste(files_path, '/emp_diversity.csv', sep = ''), skip = 8, header = TRUE)
colnames(emp_diversity) <- c('zone_name', 'geocode', 'agriculture_fishing', 'energy_water', 
                             'manufacturing', 'construction', 'motor_trades_part_G', 'wholesale_part_G',
                             'retail_part_G', 'transport_storage_H', 'accommodation_food_I',
                             'info_com_J', 'finance_K', 'property_L', 'pro_M', 'bussiness_N',
                             'public_admin_O', 'education_P', 'health_Q', 'other_services'
)

# reformat zone_name with only the name of zone without layer name by remove words before ':'
emp_diversity$zone_name <- sub('.*:', '', emp_diversity$zone_name)
emp_diversity <- emp_diversity %>% filter(emp_diversity$zone_name %in% s.geo$properties.DISPLAY_NAME) # remove unused entries

### use broad industry classification out of Standard Industrial Classification (SIC) 2007 to determine
### the diversity of employment of each zone
### i.e. A Agriculture & fishing; B,D,E Energy & water; C Manufacturing; F Construction; 
### G,I Distribution, hotels and restaurants; H,J Transport & communication; 
### K-N Banking finance & insurance etc.; O-Q Public admin education & health; R-U Other services;
### G-Q Total services

# merge values of industries
emp_diversity_GI <- rowSums(emp_diversity[, c(grep('.*[G,I]', colnames(emp_diversity)))]) # industry G,I
emp_diversity_HJ <- rowSums(emp_diversity[, c(grep('.*[H,J]', colnames(emp_diversity)))]) # industry H,J
emp_diversity_KN <- rowSums(emp_diversity[, c(grep('.*[K-N]', colnames(emp_diversity)))]) # industry K-N
emp_diversity_OQ <- rowSums(emp_diversity[, c(grep('.*[O-Q]', colnames(emp_diversity)))]) # industry O-Q

# create a new dataframe with broad industry class
broad_emp_diversity <- emp_diversity[, c('zone_name', 'geocode', 'agriculture_fishing', 
                                         'energy_water', 'manufacturing', 'construction', 
                                         'other_services')]
broad_emp_diversity$distribution_hotels <- emp_diversity_GI
broad_emp_diversity$transportation_comm <- emp_diversity_HJ
broad_emp_diversity$banking <- emp_diversity_KN
broad_emp_diversity$public_admin <- emp_diversity_OQ
broad_emp_diversity$total_emp <- rowSums(broad_emp_diversity[, 3:ncol(broad_emp_diversity)])

### calculate the entropy to determine the employment diversity (Zhong et al., 2017) 
## 1. calculate proportion of each human activity (industry) in each zone
entropy_data <- data.frame(zone_name = broad_emp_diversity$zone_name)

for(col in 3:(ncol(broad_emp_diversity) - 1)){ # col from industry A to Q
  entropy_data[, paste('p', col-2, sep = '')] <- broad_emp_diversity[, col]/broad_emp_diversity$total_emp
}

## 2. calculate K in entropy. K = 1/ln(#total activity types)
K <- 1/log(ncol(entropy_data) - 1)

## 3. calculate entropy (0 <= entropy <= 1)
# initialise the vector to store the entropy values
entropy_zone <- vector(mode = 'integer', length = nrow(entropy_data))
entropy_data[-1] <- entropy_data[-1] + 0.00001 # add noise to avoid log(0)

# calculate entropy value for each zone
for(p_col in 2:ncol(entropy_data)){ # p_col from p1 to p9
  entropy_zone <- colSums(rbind(entropy_zone, entropy_data[, p_col]*log(entropy_data[, p_col])))
}
entropy_zone <- -K*entropy_zone
entropy_data$entropy <- entropy_zone


### Main industry of each zone
# initialise a dataframe to store the main industry result
industries_name <- colnames(broad_emp_diversity[, 3:(ncol(broad_emp_diversity)-1)])
main_industry_df <- data.frame(zone_name = broad_emp_diversity$zone_name)
for(industry in industries_name){
  main_industry_df[, industry] <- 0
}

## determine the main industry/industries by allowing 5% tolerance of business register against the
## max value, i.e. the biggest proportion of industry of that zone
main_industry_tolerance <- 0.05
main_industry_indices <- apply(entropy_data[-c(1, ncol(entropy_data))], 1, 
                               function(x){which(x >= (max(x) - main_industry_tolerance))})

num_main_industries <- lengths(main_industry_indices)
# assign 1 to each zone representing the main industries of it
for(i in 1:length(main_industry_indices)){
  main_industry_df[i, c(main_industry_indices[[i]]) + 1] <- 1
}

# determine the biggest industry by proportion of human activity (industry)
biggest_industry_indices <- apply(entropy_data[-c(1, ncol(entropy_data))], 1, which.max)
biggest_industry <- industries_name[biggest_industry_indices]

main_industry_df$num_main_industries <- num_main_industries
main_industry_df$biggest_industry <- biggest_industry

##### Centrality data
centrality_data <- merge(residence_centre_data, employment_centre_data, by = 'zone_name')
centrality_data <- merge(centrality_data, entropy_data[, c('zone_name', 'entropy')], by = 'zone_name')
centrality_data <- merge(centrality_data, main_industry_df[, c('zone_name', 'num_main_industries', 
                                                               'biggest_industry')], by = 'zone_name')

cor(centrality_data[, c('density', 'backhome_prop', 'enterprise_ratio', 'entropy')]) 
# show low correlation coefficients amongst residence centre centrality & employment centre centrality


# Important Employment Centre ---------------------------------------------

### Find important employment centre -- with more commute-to-work flow

# sort employment flow in the employment centre data
sort_flow.employment_centre_data <- employment_centre_data[order(employment_centre_data$employment_flow, decreasing = TRUE),]
sort_flow.employment_centre_data <- within(sort_flow.employment_centre_data, 
                                           zone_name <- factor(zone_name, levels = zone_name))

### plot important employment centre

## bar chart of commute-to-work flow
# overview of commute-to-work flow
all_empflow <- ggplot(sort_flow.employment_centre_data, aes(x = zone_name, y = employment_flow)) + 
  geom_bar(stat = 'identity') + geom_rect(mapping=aes(xmin=0, xmax=15, ymin=0, ymax=25000), 
                                          fill = NA, color="#009E73", size = 0.3, alpha=0.5) + 
  xlab(NULL) + ylab(NULL) + theme_light() + theme(axis.text.x = element_blank())

# commute-to-work flow of focused centres with overview subplot
ggplot(sort_flow.employment_centre_data[1:15,], aes(x = zone_name, y = employment_flow)) + 
  geom_bar(stat = 'identity') + xlab('Zone') + ylab('Commute-to-work flow') + 
  geom_hline(yintercept = 8000, linetype = 'dashed', color = 'gray') + # this centre threshold covers more spatially-dispersed local centres
  annotation_custom(ggplotGrob(all_empflow), xmin = 9, xmax = 15.5, ymin = 15000, ymax = 25000) +
  theme_light() + theme(text = element_text(size = 15), axis.text.x = element_text(angle = 20), 
                        plot.margin = margin(0.2, 0.6, 0.2, 0.2, "cm"))

# ggsave('./images/select_centres.png')

## plot map of distribution of centres (bubble map)
# read shapefile of uber covered map; layer: the name of shapefile
my_spdf <- readOGR(dsn = paste0(getwd(),"/Supplementary_Data/MSOA_2011_Census_England/"), 
                    layer = "uber_movement_zones", verbose = FALSE)

# 'fortify' the data to get a dataframe format required by ggplot2
spdf_fortified <- tidy(my_spdf, region = "Zone")

# use QGIS to get the coordinates of centroids in each zone
# emp_flow.csv is compiled from 'sort_flow.employment_centre_data' with flow and coordinates info
emp_flow_withcoord <- read.csv('emp_flow.csv', header = TRUE)
# sort the data by emp_flow in order to get the zones with bigger flow presented at the front layers
emp_flow_withcoord <- emp_flow_withcoord %>% arrange(emp_flow) 


# Create breaks for the color scale
mybreaks <- c(2000, 5000, 8000, 10000, 20000)

# make bubble plot
ggplot() + geom_polygon(data = spdf_fortified, aes(x = long, y = lat, group = group), 
                        fill="grey", alpha = 0.3, color="white") +
  geom_point(data = emp_flow_withcoord, 
             aes(x = xcoord, y = ycoord, size = emp_flow, color = emp_flow, alpha = emp_flow), 
             shape = 20, stroke = FALSE) +
  # add text of centres on the map
  geom_text_repel(data = emp_flow_withcoord[emp_flow_withcoord$emp_flow > 8000,], 
                  aes(x = xcoord, y = ycoord, label = Zone), size = 3.3) +
  scale_size_continuous(name = "Commute-to-work flow", range=c(1,20), breaks = mybreaks) + 
  scale_color_viridis(option="viridis", breaks = mybreaks, name = "Commute-to-work flow") + 
  scale_alpha_continuous(name="Commute-to-work flow", range=c(0.1, .9), breaks = mybreaks) +
  theme_void() + coord_fixed(1) + guides(colour = guide_legend())

# ggsave('./images/emp_flow_map.png')


### Explore the difference among the centres

## centrality data of centres
# centre data with employment centre info
select_emp_centre_data <- sort_flow.employment_centre_data[sort_flow.employment_centre_data$employment_flow >= 8000,]
# centre data with id and zone name
centre_geo <- s.geo[s.geo$properties.DISPLAY_NAME %in% select_emp_centre_data$zone_name,]
# acquire centrality data of centres
centre_centrality_data <- centrality_data[centrality_data$zone_name %in% centre_geo$properties.DISPLAY_NAME,]

table(centrality_data$num_main_industries) # most zones have only 1 main industry
table(centrality_data$biggest_industry) # the biggest industry of most zones is banking

## Perform t-test against centres and non-centres on each variable
# first, check the assumption of t-test: normal distributed with common variance
# because Welch's t-test already handles the homogeneity of variance, we only need to check on 
# normality.

# q-q plot: especially beneficial for small sample size
plot.new()
par(mfrow=c(2,2))
for(col in c('density', 'backhome_prop', 'enterprise_ratio', 'entropy')){
  qqnorm(centre_centrality_data[, col], pch = 19)
  qqline(centre_centrality_data[, col])
}

# histogram
par(mfrow=c(2,2))
for(col in c('density', 'backhome_prop', 'enterprise_ratio', 'entropy')){
  hist(centre_centrality_data[, col])
}

# Shapiro-Wilk test
shapiro.test(centre_centrality_data$density)
shapiro.test(centre_centrality_data$backhome_prop)
shapiro.test(centre_centrality_data$enterprise_ratio)
shapiro.test(centre_centrality_data$entropy)

# Q-Q plot and histogram suggest non-normality on 'backhome_prop', 'enterprise_ratio' and 'entropy'.


## Explore residence centre centrality 

# residence density
quantile_values <- summary(centrality_data$density)[c('1st Qu.', '3rd Qu.')] # 25th and 75th percentile value of GM
ordinary_zones <- between(centre_centrality_data$density, quantile_values[1], quantile_values[2])
centre_centrality_data[!ordinary_zones, 'zone_name'] # detect which zones have much higher/lower value than that of GM

# commute-back-home flow
quantile_values <- summary(centrality_data$backhome_prop)[c('1st Qu.', '3rd Qu.')] # 25th and 75th percentile value of GM
ordinary_zones <- between(centre_centrality_data$backhome_prop, quantile_values[1], quantile_values[2])
centre_centrality_data[!ordinary_zones, 'zone_name'] # detect which zones have much higher/lower value than that of GM

## Explore employment centre centrality

# enterprise ratio
quantile_values <- summary(centrality_data$enterprise_ratio)[c('1st Qu.', '3rd Qu.')] # 25th and 75th percentile value of GM
ordinary_zones <- between(centre_centrality_data$enterprise_ratio, quantile_values[1], quantile_values[2])
centre_centrality_data[!ordinary_zones, 'zone_name'] # detect which zones have much higher/lower value than that of GM

# entropy
quantile_values <- summary(centrality_data$entropy)[c('1st Qu.', '3rd Qu.')] # 25th and 75th percentile value of GM
ordinary_zones <- between(centre_centrality_data$entropy, quantile_values[1], quantile_values[2])
centre_centrality_data[!ordinary_zones, 'zone_name'] # detect which zones have much higher/lower value than that of GM


### Examine the difference between centres and non-centres
## use logistic regression to find out significant variables which determines the differences 
## between the centres and other zones

## data preparation
centrality_data.classification <- centrality_data
# classify centres without employment flow, which is the way we define 'major employment centre'
centrality_data.classification$employment_flow <- NULL 
# create dummy variables
dum_cols <- dummy_cols(centrality_data.classification[,-c(1, 2)], remove_first_dummy = TRUE)
# add dummy variables to data
centrality_data.classification <- merge(centrality_data.classification, dum_cols, 
                                        by = colnames(centrality_data.classification[,-c(1,2)]), sort = FALSE)
centrality_data.classification$biggest_industry <- NULL # remove dummied variable
# add class to data, where class = 1 denotes this zone is the centre
centrality_data.classification$class <- 
  ifelse(centrality_data.classification$zone_name %in% centre_geo$properties.DISPLAY_NAME, 1, 0)
# standardisation
scaled.centrality_data.classification <- centrality_data.classification
scaled.centrality_data.classification[, c('density', 'backhome_prop', 'enterprise_ratio', 'entropy')] <- 
  scale(scaled.centrality_data.classification[, c('density', 'backhome_prop', 'enterprise_ratio', 'entropy')])


## Logistic Regression
# apply logistic regression for classification and find out significant variables
# first, standardise data - to see the size of effect exerted by each variable
logistic_standardreg <- glm(class ~ ., 
                            data = scaled.centrality_data.classification[, !names(scaled.centrality_data.classification) %in% c('zone_name', 'geocode')], 
                            family = 'binomial') %>% stepAIC(trace = FALSE) # use step-wise regression
summary(logistic_standardreg) # 'density', 'backhome_prop', 'enterprise_ratio', 'entropy' variables are significant
# second, the original data
logistic_reg <- glm(class ~ ., 
                    data = centrality_data.classification[, !names(centrality_data.classification) %in% c('zone_name', 'geocode')], 
                    family = 'binomial') %>% stepAIC(trace = FALSE) # use step-wise regression
summary(logistic_reg) # 'density', 'backhome_prop', 'enterprise_ratio', 'entropy' variables are significant


## Model diagnostics

## Linear Assumption (log-odds to predictors)
# Select only numeric predictors
numeric_cols <- centrality_data.classification[, names(logistic_reg$coefficients)[-1]]
predictors <- names(logistic_reg$coefficients)[-1]
probabilities <- predict(logistic_reg, type = "response")
# Bind the logit and tidying the data for plot
numeric_cols <- numeric_cols %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

# plot for linearity of each predictor
ggplot(numeric_cols, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
# it seems the predictors violate the linear assumption

## Multicollinearity
vif(logistic_reg) # if value > 5 or 10, it means there is a problem of multicollinearity

# goodness of fit test - Hosmer-Lemeshow Test. If p-value > alpha, then the model fits the observations
hoslem.test(centrality_data.classification$class, fitted(logistic_reg), g = 10)

# psudo-r2
pR2(logistic_reg)  # look for 'McFadden', 0.2-0.4 suggests an excellent fit
# ref: https://stats.stackexchange.com/questions/82105/mcfaddens-pseudo-r2-interpretation


# Accessibility Analysis on Partial/Entire GM Area ---------------------------------------------

### read required data
sex_dist <- read.xls('./Supplementary_Data/socio-economic_characteristics_GM/Socio-economic_factors.xlsx',
                     sheet = 'Sex', skip = 1, header = TRUE)
age_dist <- read.xls('./Supplementary_Data/socio-economic_characteristics_GM/Socio-economic_factors.xlsx',
                     sheet = 'Age', skip = 1, header = TRUE)
q_dist <- read.xls('./Supplementary_Data/socio-economic_characteristics_GM/Socio-economic_factors.xlsx',
                   sheet = 'Qualification', skip = 1, header = TRUE)
emp_dist <- read.xls('./Supplementary_Data/socio-economic_characteristics_GM/Socio-economic_factors.xlsx',
                     sheet = 'Employment', skip = 1, header = TRUE)
dep_dist <- read.xls('./Supplementary_Data/socio-economic_characteristics_GM/Socio-economic_factors.xlsx',
                     sheet = 'Deprivation', skip = 1, header = TRUE)
country_dist <- read.xls('./Supplementary_Data/socio-economic_characteristics_GM/Socio-economic_factors.xlsx',
                         sheet = 'Country of Birth (continent)', skip = 1, header = TRUE)
# ethnic_dist <- read.xls('./Supplementary_Data/socio-economic_characteristics_GM/Socio-economic_factors.xlsx',
#                         sheet = 'Ethnic', skip = 1, header = TRUE)
# short_res_dist <- read.xls('./Supplementary_Data/socio-economic_characteristics_GM/Socio-economic_factors.xlsx',
#                            sheet = 'Non-UK short-term resident', skip = 1, header = TRUE)
sg_dist <- read.xls('./Supplementary_Data/socio-economic_characteristics_GM/Socio-economic_factors.xlsx',
                    sheet = 'Social Grade', skip = 1, header = TRUE)
distance_dist <- read.xls('./Supplementary_Data/socio-economic_characteristics_GM/Socio-economic_factors.xlsx',
                          sheet = 'Commute Distance', skip = 1, header = TRUE)
road_dist <- read.csv('./Supplementary_Data/UK_Road_Network/GM_Network_Data.csv', header = TRUE)
distance_to_centre <- read.csv('./Supplementary_Data/distance_to_empcentres.csv', header = TRUE)

### create a dataframe with required variables

# residence centrality-related
factor_data <- centrality_data[, c('zone_name', 'density', 'backhome_prop')]
factor_data$zone_name <- as.character(factor_data$zone_name)

# male proportion
sex_dist$X.2 <- as.character(sex_dist$X.2)
sex_dist$male_prop <- sex_dist$Males/sex_dist$Total
factor_data <- merge(factor_data, sex_dist[, c('X.2', 'male_prop')], by.x = 'zone_name', by.y = 'X.2')

# working population (age 16-64) proportion
age_dist$X.2 <- as.character(age_dist$X.2)
age_dist$working_prop <- rowSums(age_dist[, 11:17])/rowSums(age_dist[, 6:ncol(age_dist)])
factor_data <- merge(factor_data, age_dist[, c('X.2', 'working_prop')], by.x = 'zone_name', by.y = 'X.2')

# proportion of 'level 4 and above' - higher education
q_dist$X.2 <- as.character(q_dist$X.2)
q_dist$l4_qualification <- q_dist$Level.4.and.above/q_dist$Total
factor_data <- merge(factor_data, q_dist[, c('X.2', 'l4_qualification')], 
                     by.x = 'zone_name', by.y = 'X.2')

# unemployment ratio
emp_dist$X.2 <- as.character(emp_dist$X.2)
emp_dist$unemployment <- emp_dist$Not.in.employment/emp_dist$Total
factor_data <- merge(factor_data, emp_dist[, c('X.2', 'unemployment')], 
                     by.x = 'zone_name', by.y = 'X.2')

# non-deprived ratio
dep_dist$X.2 <- as.character(dep_dist$X.2)
dep_dist$non_deprived <- dep_dist$Household.is.not.deprived.in.any.dimension/dep_dist$Total
factor_data <- merge(factor_data, dep_dist[, c('X.2', 'non_deprived')], 
                     by.x = 'zone_name', by.y = 'X.2')

# proportion of non-UK born individual
country_dist$X.2 <- as.character(country_dist$X.2)
country_dist$non_UK_born <- (country_dist$Total - country_dist$UK)/country_dist$Total
factor_data <- merge(factor_data, country_dist[, c('X.2', 'non_UK_born')], 
                     by.x = 'zone_name', by.y = 'X.2')

# unskilled proportion
sg_dist$X.2 <- as.character(sg_dist$X.2)
sg_dist$unskilled_prop <- sg_dist$DE.Semi.skilled.and.unskilled.manual.occupations..unemployed.and.lowest.grade.occupations/sg_dist$Total
factor_data <- merge(factor_data, sg_dist[, c('X.2', 'unskilled_prop')], 
                     by.x = 'zone_name', by.y = 'X.2')

# avg. travel distance
distance_dist$X.2 <- as.character(distance_dist$X.2)
factor_data <- merge(factor_data, distance_dist[, c('X.2', 'Avg..distance')], 
                     by.x = 'zone_name', by.y = 'X.2')

# number of major roads
num_major_roads <- data.frame(table(road_dist[road_dist$function. %in% c('A Road', 'Motorway'), 'msoa11nm']))
colnames(num_major_roads) <- c('zone_name', 'num_major_roads')
factor_data <- merge(factor_data, num_major_roads, by = 'zone_name')

# avg. distance to employment centres
mean_dist_to_centres <- aggregate(Distance ~ InputID, data = distance_to_centre, mean) # calculate average distance to all centres
colnames(mean_dist_to_centres) <- c('zone_name', 'mean_distance_to_centres')
factor_data <- merge(factor_data, mean_dist_to_centres, by = 'zone_name', all.x = TRUE)

## remove centre data for analysis
nocentre_factor_data <- factor_data[!factor_data$zone_name %in% centre_geo$properties.DISPLAY_NAME,] 

## examine correlation between features
cor(nocentre_factor_data[,-1]) 
# indicate 'l4_qualification', 'unemployment', 'non-deprived', 'unskilled_prop' have high association with one another
# keep 'non-deprived' as representative and dismiss the other correlated variables
indep_factor_data <- nocentre_factor_data[, ! names(nocentre_factor_data) %in% c('l4_qualification', 'unemployment', 'unskilled_prop')]
cor(indep_factor_data[,-1]) # no high association shown
str(indep_factor_data) # check type of each column

### Why do some zones have far higher accessibility to multiple centres than the others?
### Specifically, they are located in the middle of those centres, especially Trafford 008, 
### 011, 012, 014, 018 and Manchester 037, 039, 042, 043, 044, 046

# tag those zones as class 1, and assign 0 to the rest
indep_factor_data.classification <- indep_factor_data
accessible_to_multiple_centres <- c('Trafford 008', 'Trafford 011', 'Trafford 012', 
                                    'Trafford 014', 'Trafford 018', 'Manchester 037', 
                                    'Manchester 039', 'Manchester 042', 'Manchester 043', 
                                    'Manchester 044', 'Manchester 046')
indep_factor_data.classification$class <- ifelse(indep_factor_data.classification$zone_name %in% 
                                                   accessible_to_multiple_centres, 1, 0)
# standardisation
scaled.indep_factor_data.classification <- indep_factor_data.classification
scaled.indep_factor_data.classification[, !names(scaled.indep_factor_data.classification) %in% c('zone_name', 'class')] <- 
  scale(scaled.indep_factor_data.classification[, !names(scaled.indep_factor_data.classification) %in% c('zone_name', 'class')])

# information of zones belong to class 1
class1_zones <- factor_data[factor_data$zone_name %in% accessible_to_multiple_centres, ]

### Logistic Regression
# apply logistic regression for classification and find out significant variables
# first, standardise data - to see the size of effect exerted by each variable
logistic_standardreg <- glm(class ~ ., 
                            data = scaled.indep_factor_data.classification[, !names(scaled.indep_factor_data.classification) %in% c('zone_name')], 
                            family = 'binomial') %>% stepAIC(trace = FALSE) # use step-wise regression
summary(logistic_standardreg) # 'density', 'backhome_prop', 'enterprise_ratio', 'entropy' variables are significant

# second, the original data
logistic_reg <- glm(class ~ ., 
                    data = indep_factor_data.classification[, !names(indep_factor_data.classification) %in% c('zone_name')], 
                    family = 'binomial') %>% stepAIC(trace = FALSE)
summary(logistic_reg) # 'density', 'non_deprived' and 'mean_distance_to_centres' are significant variables

### Model diagnostics

## Linear Assumption (log-odds to predictors)
# Select only numeric predictors
numeric_cols <- indep_factor_data.classification[, names(logistic_reg$coefficients)[-1]]
predictors <- names(logistic_reg$coefficients)[-1]
probabilities <- predict(logistic_reg, type = "response")
# Bind the logit and tidying the data for plot
numeric_cols <- numeric_cols %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

# plot for linearity of each predictor
ggplot(numeric_cols, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
# it seems the predictors violate the linear assumption

## Multicollinearity
vif(logistic_reg) # if value > 5 or 10, it means there is a problem of multicollinearity

# goodness of fit test - Hosmer-Lemeshow Test. If p-value > alpha, then the model fits the observations
hoslem.test(indep_factor_data.classification$class, fitted(logistic_reg), g=10)

# psudo-r2
pR2(logistic_reg)  # look for 'McFadden'


### Why do some zones located on the belt to centres have lower accessibility than the other counterpart?
### When travelling about 30 mins, most of the central zones can access to all the employment centres,
### while some of them, which are even on the belts to centres, have lower accessibility than others.
### Specifically, Salford 021, Trafford 006; Manchester 052; Tameside 010, 011, 012, 
### Manchester 012, 017, 023, Stockport 001, 002, 003.
### Manchester 028 is located in the central area but shows low accessibility as well.

# central zones
# Trafford 001, 003, 004, 006, 008, 009, 011, 012, 014, 018, 028, Salford 021, 
# Manchester 012, 013, 015, 017, 019, 020-052, 057, 059, Stockport 001-004, 006-009, 011, 013,
# 015, 016, 018, 019, 022, 023, 025, 028, 030, 036, Tameside 010-012, 014, 024-025
central_zones <- c('Trafford 001', 'Trafford 003', 'Trafford 004', 'Trafford 006', 
                   'Trafford 008', 'Trafford 009', 'Trafford 011', 'Trafford 012', 'Trafford 014', 
                   'Trafford 018', 'Trafford 028', 'Salford 021', 'Manchester 012', 
                   'Manchester 013', 'Manchester 015', 'Manchester 017', 'Manchester 019', 
                   paste('Manchester 0', 20:52, sep = ''), 'Manchester 057', 'Manchester 059',
                   paste('Stockport 00', 1:4, sep = ''), paste('Stockport 00', 6:9, sep = ''), 
                   'Stockport 011', 'Stockport 013', 'Stockport 015', 'Stockport 016', 
                   'Stockport 018', 'Stockport 019',  'Stockport 022', 'Stockport 023', 
                   'Stockport 025', 'Stockport 028', 'Stockport 030', 'Stockport 036', 
                   paste('Tameside 0', 10:12, sep = ''), 'Tameside 014', 'Tameside 024', 
                   'Tameside 025')

indep_factor_data.central <- indep_factor_data[indep_factor_data$zone_name %in% central_zones,]

# tag those zones as class 1, and assign 0 to the rest
indep_factor_data.classification <- indep_factor_data.central
centralzones_low_accessibility <- c('Salford 021', 'Trafford 006', 'Manchester 052', 
                                    'Tameside 010', 'Tameside 011', 'Tameside 012', 
                                    'Manchester 012', 'Manchester 017', 'Manchester 023', 
                                    'Stockport 001', 'Stockport 002', 'Stockport 003',
                                    'Manchester 028')
indep_factor_data.classification$class <- ifelse(indep_factor_data.classification$zone_name %in% 
                                                   centralzones_low_accessibility, 1, 0)

# standardisation
scaled.indep_factor_data.classification <- indep_factor_data.classification
scaled.indep_factor_data.classification[, !names(scaled.indep_factor_data.classification) %in% c('zone_name', 'class')] <- 
  scale(scaled.indep_factor_data.classification[, !names(scaled.indep_factor_data.classification) %in% c('zone_name', 'class')])

# information of zones belong to class 1
class1_zones <- factor_data[factor_data$zone_name %in% centralzones_low_accessibility, ]

### Logistic Regression
# apply logistic regression for classification and find out significant variables
# first, standardise data - to see the size of effect exerted by each variable
logistic_standardreg <- glm(class ~ ., 
                            data = scaled.indep_factor_data.classification[, !names(scaled.indep_factor_data.classification) %in% c('zone_name')], 
                            family = 'binomial') %>% stepAIC(trace = FALSE) # use step-wise regression
summary(logistic_standardreg) # 'density', 'backhome_prop', 'enterprise_ratio', 'entropy' variables are significant

# second, the original data
logistic_reg <- glm(class ~ ., 
                    data = indep_factor_data.classification[, !names(indep_factor_data.classification) %in% c('zone_name')], 
                    family = 'binomial') %>% stepAIC(trace = FALSE)
summary(logistic_reg) # 'non_deprived', 'non_UK_born' and 'mean_distance_to_centres' are significant variables


### Model diagnostics

## Linear Assumption (log-odds to predictors)
# Select only numeric predictors
numeric_cols <- indep_factor_data.classification[, names(logistic_reg$coefficients)[-1]]
predictors <- names(logistic_reg$coefficients)[-1]
probabilities <- predict(logistic_reg, type = "response")
# Bind the logit and tidying the data for plot
numeric_cols <- numeric_cols %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

# plot for linearity of each predictor
ggplot(numeric_cols, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
# it seems the predictors violate the linear assumption

## Multicollinearity
vif(logistic_reg) # if value > 5 or 10, it means there is a problem of multicollinearity

# goodness of fit test - Hosmer-Lemeshow Test. If p-value > alpha, then the model fits the observations
hoslem.test(indep_factor_data.classification$class, fitted(logistic_reg), g=10)

# psudo-r2
pR2(logistic_reg)  # look for 'McFadden'


## construct a stacked bar chart of CoB on central zones with low and high accessibility level 
## as well as non-centres
## first, arrange the data to a proper form (long table)

# group sum for each CoB continent data
centralzones_low_immigrants <- colSums(country_dist[country_dist$X.2 %in% centralzones_low_accessibility, 
                                                    c('Total', 'UK', 'Other.Europe', 'Africa', 
                                                      'Middle.East.and.Asia', 
                                                      'The.Americas.and.the.Caribbean')])

centralzones_high_accessibility <- setdiff(central_zones, centralzones_low_accessibility)
centralzones_high_immigrants <- colSums(country_dist[country_dist$X.2 %in% centralzones_high_accessibility, 
                                                     c('Total', 'UK', 'Other.Europe', 'Africa', 
                                                       'Middle.East.and.Asia', 
                                                       'The.Americas.and.the.Caribbean')])

noncentre_immigrants <- colSums(country_dist[country_dist$X.2 %in% indep_factor_data$zone_name, 
                                             c('Total', 'UK', 'Other.Europe', 'Africa', 
                                               'Middle.East.and.Asia', 
                                               'The.Americas.and.the.Caribbean')])

# initialise variables to store the results
centralzones_low_prop_immigrants <- vector(mode = 'numeric')
centralzones_high_prop_immigrants <- vector(mode = 'numeric')
noncentre_prop_immigrants <- vector(mode = 'numeric')

# calculate the percentage of CoB data
for(i in 2:6){
  centralzones_low_prop_immigrants <- c(centralzones_low_prop_immigrants,
                                        centralzones_low_immigrants[i]/centralzones_low_immigrants[1])
  centralzones_high_prop_immigrants <- c(centralzones_high_prop_immigrants,
                                         centralzones_high_immigrants[i]/centralzones_high_immigrants[1])
  noncentre_prop_immigrants <- c(noncentre_prop_immigrants,
                                 noncentre_immigrants[i]/noncentre_immigrants[1])
}

# arrange the data to a proper form
immigrants_data <- data.frame(area = rep(c('Central (low)', 'Central (high)', 'Non-centres'), each = 5),
                              continent = rep(gsub("\\.", " ", names(centralzones_low_prop_immigrants)), 3), # replace '.' with space
                              percentage = c(centralzones_low_prop_immigrants, centralzones_high_prop_immigrants, noncentre_prop_immigrants))

## Second, construct stacked bar chart
ggplot(immigrants_data, aes(fill = continent, y = percentage, x = area)) + 
  geom_bar(position="fill", stat="identity") + xlab('Area') + ylab('Proportion (%)') + 
  labs(fill = 'Continent') + theme_light() + 
  theme(text = element_text(size = 15), axis.title.x = element_text(vjust = -1.5), 
        axis.title.y = element_text(vjust = 3), plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "cm"))

# ggsave('./images/immigrants_ratio.png')


### Why some zones fail to reach any centre in 30 mins?
### e.g. Bury 009, 014, 015, 016, 018, Rochdale 006, 007, 009, 011, 013, 018, Oldham 003, 004, 007

# tag those zones as class 1, and assign 0 to the rest
indep_factor_data.classification <- indep_factor_data
zones_over30 <- c('Bury 009', 'Bury 014', 'Bury 015', 'Bury 016', 'Bury 018',
                  'Rochdale 006', 'Rochdale 007', 'Rochdale 009', 'Rochdale 011', 'Rochdale 013',
                  'Rochdale 018', 'Oldham 003', 'Oldham 004', 'Oldham 007')
indep_factor_data.classification$class <- ifelse(indep_factor_data.classification$zone_name %in% 
                                                   zones_over30, 1, 0)

# standardisation
scaled.indep_factor_data.classification <- indep_factor_data.classification
scaled.indep_factor_data.classification[, !names(scaled.indep_factor_data.classification) %in% c('zone_name', 'class')] <- 
  scale(scaled.indep_factor_data.classification[, !names(scaled.indep_factor_data.classification) %in% c('zone_name', 'class')])

# information of zones belong to class 1
class1_zones <- factor_data[factor_data$zone_name %in% zones_over30, ]

### Logistic Regression
# apply logistic regression for classification and find out significant variables
# first, standardise data - to see the size of effect exerted by each variable
logistic_standardreg <- glm(class ~ ., 
                            data = scaled.indep_factor_data.classification[, !names(scaled.indep_factor_data.classification) %in% c('zone_name')], 
                            family = 'binomial') %>% stepAIC(trace = FALSE) # use step-wise regression
summary(logistic_standardreg) # 'mean_distance_to_centres', 'working_prop', 'Avg..distance' variables are significant

# second, the original data
logistic_reg <- glm(class ~ ., 
                    data = indep_factor_data.classification[, !names(indep_factor_data.classification) %in% c('zone_name')], 
                    family = 'binomial') %>% stepAIC(trace = FALSE)
summary(logistic_reg)


### Model diagnostics

## Linear Assumption (log-odds to predictors)
# Select only numeric predictors
numeric_cols <- indep_factor_data.classification[, names(logistic_reg$coefficients)[-1]]
predictors <- names(logistic_reg$coefficients)[-1]
probabilities <- predict(logistic_reg, type = "response")
# Bind the logit and tidying the data for plot
numeric_cols <- numeric_cols %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

# plot for linearity of each predictor
ggplot(numeric_cols, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
# it seems the predictors violate the linear assumption

## Multicollinearity
vif(logistic_reg) # if value > 5 or 10, it means there is a problem of multicollinearity

# goodness of fit test - Hosmer-Lemeshow Test. If p-value > alpha, then the model fits the observations
hoslem.test(indep_factor_data.classification$class, fitted(logistic_reg), g=10)
# psudo-r2
pR2(logistic_reg)  # look for 'McFadden'


## construct horizontal bar chart for distances (kms)
# arrange data to a proper form
distance_central <- c(mean(indep_factor_data.central$Avg..distance),
                      mean(indep_factor_data.central$mean_distance_to_centres/1000)) # convert to km

distance_noncentres <- c(mean(indep_factor_data$Avg..distance),
                         mean(indep_factor_data$mean_distance_to_centres/1000)) # convert to km

distance_zones30 <- c(mean(class1_zones$Avg..distance),
                      mean(class1_zones$mean_distance_to_centres/1000)) # convert to km

distance_data <- data.frame(area = rep(c('Central', 'Non-centres', 'Zones > 30'), each = 2),
                            variable = rep(c('Average commute distance', 'Average distance to centres'), 3),
                            value = c(distance_central, distance_noncentres, distance_zones30))

# plot horizontal bar chart
ggplot(data = distance_data, aes(x = area, y = value, fill = variable)) +
  geom_bar(stat = "identity", position=position_dodge()) + xlab ('Area') + ylab('Distance (kms)') +
  theme_light() + theme(text = element_text(size = 15), legend.position="top", 
                        axis.title.x = element_text(vjust = -1.5), 
                        axis.title.y = element_text(vjust = 3), 
                        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "cm")) + coord_flip()
# ggsave('./images/distance.png')

## make flow plot to alternative centres
# data preparation
flow_graph <- data.frame(source = rep(od_zone, each = length(od_zone)), 
                         dst = rep(od_zone, length(od_zone)), 
                         flow_weight = as.vector(t(od_flow)))

flow_zone30 <- flow_graph[flow_graph$source %in% zones_over30, ]
flow50_zone30 <- merge(flow_zone30[flow_zone30$flow_weight > 50,], # only plot flow > 50
                        emp_flow_withcoord[, c('Zone', 'xcoord', 'ycoord')], # add coordinates info for source
                       by.x = 'source', by.y = 'Zone')
flow50_zone30 <- merge(flow50_zone30, 
                        emp_flow_withcoord[, c('Zone', 'xcoord', 'ycoord')],  # add coordinates info for destination
                        by.x = 'dst', by.y = 'Zone')
flow50_zone30 <- flow50_zone30[flow50_zone30$source != flow50_zone30$dst,] # remove flow of self-loop

# sort the data by flow_weight in order to get the zones with bigger flow presented at the front layers
flow50_zone30 <- flow50_zone30 %>% arrange(flow_weight)

# plot flow map
ggplot() + geom_polygon(data = spdf_fortified, aes(x = long, y = lat, group = group), 
                        fill="grey", alpha = 0.3, color="white") + coord_fixed(1) + 
  geom_curve(data = flow50_zone30, curvature = 0.2,
             aes(x = xcoord.x, y = ycoord.x, xend = xcoord.y, yend = ycoord.y, 
                 color = flow_weight, size = flow_weight), 
             arrow = arrow(length = unit(0.02, "npc"))) + 
  # add labels of alternative centres
  geom_label_repel(data = unique(flow50_zone30[flow50_zone30$flow_weight > 200, 
                                                c('dst', 'xcoord.y', 'ycoord.y')]), 
                   aes(x = xcoord.y, y = ycoord.y, label = dst), 
                   fill = alpha(c("white"), 0.8), label.size = NA) +
  scale_size_continuous(name = "Commute-to-work flow", range=c(0.5, 2)) + 
  scale_color_viridis(option="viridis", name = "Commute-to-work flow") + 
  scale_alpha_continuous(name = "Commute-to-work flow", range=c(0.1, .9)) +
  theme_void() + #theme(legend.title=element_text(size = 15), legend.text=element_text(size = 14)) + 
  guides(colour = guide_legend())

# ggsave('./images/alternative_centres_zones30.png')


# Accessibility Level Classification --------------------------------------------

### prepare data for the model
centre1 <- c('Manchester 054', 'Manchester 055', 'Manchester 060', 'Salford 028', 'Manchester 018')
centre2 <- c('Manchester 053')
centre3 <- c('Stockport 014')
centre4 <- c('Tameside 013')
centre5 <- c('Trafford 002')

### Travel times to the important employment centres during 7-10 AM peak
# extract travel time data during 7-10 AM
traveltime_AM <- wd[(wd$hod >= 7) & (wd$hod <= 10), c('time', 'sourceid', 'dstid', 'mean_travel_time')]
# average travel times over 2016Q1-2019Q3
traveltime_AM_noquarter <- aggregate(mean_travel_time ~ sourceid + dstid, 
                                     data = traveltime_AM, mean)

# acquire travel times to employment centres
time_to_centre <- traveltime_AM_noquarter[traveltime_AM_noquarter$dstid %in% centre_geo$properties.MOVEMENT_ID,]

### Cumulative method
## Use time threshold: 5, 10, 15, ..., 60 mins

accessible_zone <- function(min_threshold){
  accessible_zones <- time_to_centre[time_to_centre$mean_travel_time <= min_threshold*60,]
  accessible_zones <- merge(accessible_zones, s.geo[, 1:2], by.x = 'sourceid', 
                            by.y = 'properties.MOVEMENT_ID')
  accessible_zones <- merge(accessible_zones, s.geo[, 1:2], by.x = 'dstid', 
                            by.y = 'properties.MOVEMENT_ID')
  colnames(accessible_zones)[4:5] <- c('sourcename', 'dstname')
  accessible_zones <- accessible_zones[, c(2, 1, 3:ncol(accessible_zones))]
  return(accessible_zones)
}

# min_threshold <- 45 # after 45, the result fixed
# result <- accessible_zone(min_threshold)

# compute the number of centres is accessible within t mins for zones, 
# which is our class variable (y) in the model
access_classification_data <- data.frame()

for(t in seq(5, 50, 5)){
  accessibility_data <- accessible_zone(t)
  accessibility_data$centreid <- 0
  
  accessibility_data[accessibility_data$dstname %in% centre1, 'centreid'] <- 1
  accessibility_data[accessibility_data$dstname %in% centre2, 'centreid'] <- 2
  accessibility_data[accessibility_data$dstname %in% centre3, 'centreid'] <- 3
  accessibility_data[accessibility_data$dstname %in% centre4, 'centreid'] <- 4
  accessibility_data[accessibility_data$dstname %in% centre5, 'centreid'] <- 5
  
  access_numcentre_data <- data.frame(table(unique(accessibility_data[, c('sourcename', 'centreid')])$sourcename))
  access_numcentre_data$mins <- t
  access_classification_data <- rbind(access_classification_data, data.frame(access_numcentre_data))
}

colnames(access_classification_data)[1:2] <- c('zone_name', 'num_centres')

### plot accessibility level change with stacked area chart

## arrange data to a proper form (long table)
all_access_count_data <- data.frame(table(access_classification_data[, c('mins', 'num_centres')]))
all_access_count_data$mins <- unfactor(all_access_count_data$mins)
all_access_count_data$num_centres <- unfactor(all_access_count_data$num_centres)

for(t in seq(5, 50, 5)){
  num_centre0 <- 246 - sum(all_access_count_data[all_access_count_data$mins == t, 'Freq'])
  all_access_count_data <- rbind(all_access_count_data, c(t, 0, num_centre0))
}

all_access_count_data$num_centres <- as.factor(all_access_count_data$num_centres)
# calculate proportion for the area chart
all_access_count_data$proportion <- all_access_count_data$Freq/nrow(s.geo)

## plot stacked area chart
ggplot(all_access_count_data, aes(x = mins, y = proportion, fill = num_centres)) + 
  geom_area(alpha = 0.75, size = 1) + xlab('Time threshold (mins)') + ylab('Proportion (%)') +
  labs(fill = "Accessibility level") + theme_minimal() + 
  theme(text = element_text(size = 15), axis.title.x = element_text(vjust = -1.5), 
        axis.title.y = element_text(vjust = 3), plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "cm"))
# ggsave('./images/accessibility_level_change.png')

### build xgboost classifier
# apply xgboost for multi-class classification and find out variable importance
# Code Ref: https://rpubs.com/mharris/multiclass_xgboost
# Useful ref: https://www.hackerearth.com/zh/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/
# parameter ref: https://stackoverflow.com/questions/54565265/how-to-understand-nfold-and-nrounds-in-rs-package-xgboost

xgboost_model <- function(data, nround = 5, cv.nfold = 5){
  
  # convert data to matrix for xgboost
  data_variables <- as.matrix(data[, !names(data) %in% c('zone_name', 'num_centres')])
  data_label <- data[, "num_centres"]
  data_matrix <- xgb.DMatrix(data = data_variables, label = data_label)
  
  # construct xgboost model
  numberOfClasses <- length(unique(data_label))
  xgb_params <- list("objective" = "multi:softprob", # obtain the predicted prob. of each class
                     "eval_metric" = "mlogloss",
                     "num_class" = numberOfClasses)
  
  # Fit cv.nfold * cv.nround XGB models and save OOF predictions
  cv_model <- xgb.cv(params = xgb_params, data = data_matrix, 
                     nrounds = nround, nfold = cv.nfold, stratified = TRUE,
                     verbose = FALSE, prediction = TRUE)
  
  # OOF prediction error
  OOF_prediction <- data.frame(cv_model$pred) %>% 
    mutate(max_prob = max.col(., ties.method = "last"), 
           # add col 'max_prob' to indicate the predicted class
           label = data_label + 1)
  
  # confusion matrix (need to feed factors to the function)
  OOF_prediction$max_prob <- as.factor(OOF_prediction$max_prob)
  OOF_prediction$label <- as.factor(OOF_prediction$label)
  levels(OOF_prediction$max_prob) <- levels(OOF_prediction$label) # ensure factor level is the same
  
  classification_result <- confusionMatrix(OOF_prediction$max_prob, OOF_prediction$label, 
                                           mode = "everything")
  accuracy <- classification_result$overall[1] # get training accuracy from k-fold cv
  
  # Variable Importance
  bst_model <- xgb.train(params = xgb_params, data = data_matrix, nrounds = nround) # train the model
  
  names <-  colnames(data[, !names(data) %in% c('zone_name')])
  importance_matrix <- xgb.importance(feature_names = names, model = bst_model)
  
  plot_var_importance <- xgb.ggplot.importance(importance_matrix) # cluster the importance of variables
  var_importance <- data.frame(cbind(plot_var_importance$data$Feature, 
                                     plot_var_importance$data$Importance, 
                                     plot_var_importance$data$Cluster))
  
  return(list(accuracy = accuracy, var_importance = var_importance))
  
}

### Loop through 5-45 mins threshold to get the classification result and variable importance

# initialise variables to store the results
set.seed(518492)
xg_accuracy <- NULL
xg_var_imp <- data.frame()

for(t in seq(5, 45, 5)){
  
  # main data
  indep_factor_data.classification <- indep_factor_data
  
  # add num_centre class label to our data based on time threshod 
  xg_data <- access_classification_data[access_classification_data$mins == t,]
  indep_factor_data.classification <- merge(indep_factor_data.classification, xg_data[,c('zone_name', 'num_centres')], 
                                            by = 'zone_name', all.x = TRUE)
  indep_factor_data.classification$num_centres[is.na(indep_factor_data.classification$num_centres)] <- 0
  
  # run model and get the classification results
  xg_result <- xgboost_model(indep_factor_data.classification)
  # save the results
  xg_accuracy <- c(xg_accuracy, xg_result$accuracy)
  var_importance <- data.frame(xg_result$var_importance)
  var_importance$mins <- t
  var_importance$model <- t/5
  xg_var_imp <- rbind(xg_var_imp, var_importance)
  
}


# As it turns out 'mean_distance_to_centres' accounts for the major part of variable importance
# in classification. 'non_deprived', 'Avg..distance' and 'backhome_prop' presents the second major part at 10 mins

### plot the accuracy result with bar chart
accuracy_data <- data.frame(mins = seq(5, 45, 5), accuracy = xg_accuracy)
ggplot(accuracy_data, aes(x = mins, y = accuracy)) + geom_bar(stat="identity") + geom_point() +
  geom_line(data = accuracy_data, aes(x = mins, y = accuracy), size = 1.1) + 
  xlab('Time threshold (mins)') + ylab('Accuracy') + theme_light() + 
  theme(text = element_text(size = 17), axis.title.x = element_text(vjust = -1.5), 
        axis.title.y = element_text(vjust = 3), plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "cm"))

# ggsave('./images/xgboost_accuracy.png')

### plot the feature importance result with radar chart

## arrange data to a proper form (wide table)
# pick model with decent accuracy (>=0.6) for the plot
var_imp_data <- xg_var_imp[xg_var_imp$model %in% which(xg_accuracy >= 0.6),]

# arrange data
var_imp_data$X2 <- unfactor(var_imp_data$X2)

var_imp_radar <- data.frame(matrix(ncol = length(unique(var_imp_data[var_imp_data$X2 >= 0.1, 'X1'])), # only keep variables which their importance is > 0.1
                                   nrow = length(unique(var_imp_data$model))+2), 
                            row.names = c(1, 2, unique(var_imp_data$mins))) # radar chart needs this format for first 2 rows 
colnames(var_imp_radar) <- unique(var_imp_data[var_imp_data$X2 >= 0.1, 'X1'])

for(i in 1:ncol(var_imp_radar)){ # keep first 2 rows for radar chart building (1st: upper limit; 2nd:lower limit)
  var_imp_radar[3:nrow(var_imp_radar), i] <- round(var_imp_data[var_imp_data$X1 == colnames(var_imp_radar)[i], 'X2']*100)
}

# upper limit and lower limit
upper_limit <- round_any(max(var_imp_radar, na.rm = TRUE), 10, f = ceiling) # get the nearest power of 10
var_imp_radar[1, ] <- upper_limit
var_imp_radar[2,] <- 0

## plot radar chart
# Color vector
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )

dev.off()
# png('./images/radar_chart.png', width=6,height=6,units='in', res=300)
par(xpd = TRUE, mar=c(0, 3, 0, 4.5))
radarchart(var_imp_radar, axistype=1, 
           #custom polygon
           pcol=colors_border, pfcol=colors_in, plwd = 3, plty = 1,
           #custom the grid
           cglcol="grey", cglty=1, axislabcol="grey", 
           caxislabels = seq(0, upper_limit, upper_limit/4), cglwd = 1,
           #custom labels
           vlcex = 1, vlabels = c('avg. distance to centres', 'commute-back-home',
                                  'density', 'non-deprived', 'avg. commute distance')
)

# Add a legend
legend(x = 1.1, y = 1.2, legend = rownames(var_imp_radar[-c(1,2),]), 
       title = "Time threshold", bty = "n", pch = 20 , 
       col = colors_in , text.col = "grey", cex = 1, pt.cex = 3)

# dev.off()


# Travel times time series ------------------------------------------------

### Travel time trend to each centre area
## data preparation
centres <- list(centre1, centre2, centre3, centre4, centre5)
# add centre area id to centre_geo
for(i in 1:length(centres)){
  centre_geo[centre_geo$properties.DISPLAY_NAME %in% centres[[i]], 'centre_area'] <- i
}

# construct list with each element containing time series and average travel times to certain centre area
centre_quarterly_time_list <- list()
for(i in 1:length(centres)){
  time_to_centre <- traveltime_AM[traveltime_AM$dstid %in% centre_geo[centre_geo$centre_area == i, 'properties.MOVEMENT_ID'],]
  traveltime_AM_quarter <- aggregate(mean_travel_time ~ time, data = time_to_centre, mean)
  
  traveltime_AM_quarter$centre <- i
  centre_quarterly_time_list[[i]] <- traveltime_AM_quarter
}

# combine the list elements
centre_quarterly_time <- do.call(rbind, centre_quarterly_time_list)
centre_quarterly_time$time <- unfactor(centre_quarterly_time$time)
centre_quarterly_time$centre <- as.factor(centre_quarterly_time$centre)

## plot line plot for time series

# labels for xtick
quarters <- c('2016Q1', rep('', 3), '2017Q1', rep('', 3), '2018Q1', rep('', 3), '2019Q1', rep('', 2))

ggplot() +
  geom_line(data = centre_quarterly_time, aes(x = time, y = mean_travel_time, group = centre, color = centre),
            lwd = 1.4) +
  xlab('Time (year-quarter)') + ylab('Average travel times (secs)') + labs(color = "Centre area") + 
  scale_x_continuous(n.breaks = 15, labels = quarters) +
  scale_color_viridis(labels = c('Manchester city centre', 'Manchester Airport', 'Stockport',
                                 'Tameside', 'Trafford'), discrete = TRUE) + 
  theme_light() + theme(legend.position = "bottom", text = element_text(size = 16), 
                        axis.title.x = element_text(vjust = -1.5), 
                        axis.text = element_text(size = 12),
                        axis.title.y = element_text(vjust = 3), 
                        plot.margin = unit(c(0.5, 2.5, 0.5, 0.8), "cm"))

# ggsave('./images/travel_times_TS.png')


# Accessibility Level Time Series -----------------------------------------

time_to_centre <- traveltime_AM[traveltime_AM$dstid %in% centre_geo$properties.MOVEMENT_ID,]
time_to_centre <- merge(time_to_centre, centre_geo[, c('properties.MOVEMENT_ID', 'centre_area')], 
                        by.x = 'dstid', by.y = 'properties.MOVEMENT_ID')

# construct list with each element containing time series and accessibility level at x time threshold
centre_quarterly_access_list <- list()
minutes <- c(15, 20, 30) # time threshold

for(i in 1:length(minutes)){
  # select travel times <= time threshold
  threshold.time_to_centre <- time_to_centre[time_to_centre$mean_travel_time <= 60*minutes[i],]
  # find each zone per quarter can access how many centre areas within time threshold
  threshold.time_to_centre <- aggregate(centre_area ~ time + sourceid, data = threshold.time_to_centre, 
                                        function(x) length(unique(x)))
  
  ## assign 0 centre area that zones can access within time threshold for those not in 'threshold.time_to_centre'
  zero_centre <- unique(time_to_centre[!time_to_centre$sourceid %in% threshold.time_to_centre$sourceid, 'sourceid'])
  zero_centre <- data.frame(cbind(unfactor(rep(unique(threshold.time_to_centre$time), each = length(zero_centre))), zero_centre, 0))
  colnames(zero_centre) <- colnames(threshold.time_to_centre) # to match the colnames of threshold.time_to_centre for further combination
  threshold.time_to_centre <- rbind(threshold.time_to_centre, zero_centre)
  
  if(minutes[i] == 30){access_ts_30 <- threshold.time_to_centre} # output the results at 30 minutes threshold for further analysis
  
  # compute average accessibility level per quarter
  threshold.time_to_centre <- aggregate(centre_area ~ time, data = threshold.time_to_centre, mean)
  threshold.time_to_centre$threshold <- minutes[i]
  
  centre_quarterly_access_list[[i]] <- threshold.time_to_centre
}

# combine elements in the list
centre_quarterly_access_list <- do.call(rbind, centre_quarterly_access_list)
centre_quarterly_access_list$time <- unfactor(centre_quarterly_access_list$time)
centre_quarterly_access_list$threshold <- as.factor(centre_quarterly_access_list$threshold)

### plot line plot for time series

# labels for xtick
quarters <- c('2016Q1', rep('', 3), '2017Q1', rep('', 3), '2018Q1', rep('', 3), '2019Q1', rep('', 2))

# make line plot
ggplot() +
  geom_line(data = centre_quarterly_access_list, aes(x = time, y = centre_area, group = threshold, color = threshold),
            lwd = 1.4) +
  xlab('Time (year-quarter)') + ylab('Accessibility level') + labs(color = "Time threshold (mins)") + 
  scale_x_continuous(n.breaks = 15, labels = quarters) +
  theme_light() + theme(legend.position = "bottom", text = element_text(size = 18), 
                        axis.title.x = element_text(vjust = -1.5), 
                        axis.text = element_text(size = 13),
                        axis.title.y = element_text(vjust = 3), 
                        plot.margin = unit(c(0.2, 0.2, 0, 0.4), "cm")
  )

# ggsave('./images/accessibility_TS.png')

# Future change on accessibility level ------------------------------------

### Find zones appear to decrease in travel time to centre areas

## Use time series clustering to find cluster with decreasing trend

# data preparation
# construct time series of average travel times to each centre area
zone_traveltime_change <- list()
for(i in 1:length(centres)){
  time_to_centre <- traveltime_AM[traveltime_AM$dstid %in% centre_geo[centre_geo$centre_area == i, 'properties.MOVEMENT_ID'],]
  time_to_centre <- aggregate(mean_travel_time ~ time + sourceid, data = time_to_centre, mean)
  time_to_centre$centre <- i
  zone_traveltime_change[[i]] <- time_to_centre
}

zone_traveltime_change <- do.call(rbind, zone_traveltime_change)

# only cluster series having travel times recorded for over half the period, i.e. 7.5 quarters
length_ts_zone <- table(zone_traveltime_change$sourceid) # get the length of time series for each zone
zone_traveltime_change <- zone_traveltime_change[zone_traveltime_change$sourceid %in% 
                                                   names(which(length_ts_zone >= 15/2)),]

# initialise a variable to store the results
decreasing_cluster_diffmaxmin <- list()

for(centre in 1:length(centres)){
  # select one centre area
  centre_cluster_change <- zone_traveltime_change[zone_traveltime_change$centre == centre,]
  
  # arrange data for time series clustering
  # convert travel times to a list based on their zones
  df_list <- as.list(utils::unstack(zone_traveltime_change, mean_travel_time ~ sourceid))
  # data standardisation
  df_list_z <- dtwclust::zscore(df_list)
  
  # hierarchical clustering for up to k=10 clusters
  num_k <- 10
  cluster_dtw_h <- tsclust(df_list_z, type = "h", k = 2:num_k,  distance = "dtw", 
                           control = hierarchical_control(method = "complete"), seed = 518492, 
                           preproc = NULL)
  
  # use CVIs to choose the optimal number of clusters
  CVIs <- lapply(cluster_dtw_h, cvi, type = "internal")
  CVIs <- data.frame(k = rep(c(1:(num_k-1)), each = length(CVIs[[1]])),
                     metrics = rep(names(CVIs[[1]]), num_k-1),
                     value = unlist(CVIs))
  ggplot() + geom_line(data = CVIs, aes(x = k, y = value, group = metrics, color = metrics)) + 
    theme_light()
  
  # time series cluster plot
  # png('./images/cluster_sc.png', width=12,height=4,units='in', res=300)
  plot(cluster_dtw_h[[1]], type = "sc")
  # dev.off()
  
  # the representative prototype 
  # png('./images/cluster_centroid.png', width=12,height=4,units='in', res=300)
  plot(cluster_dtw_h[[1]], type = "centroid")
  # dev.off()
  
  # zones which belongs to the decreasing trend cluster
  centre_cluster_change <- centre_cluster_change[centre_cluster_change$sourceid %in% 
                                                   as.numeric(names(which(cluster_dtw_h[[3]]@cluster == 4))),]
  
  # the degree of travel time decrease
  max_min_difference <- aggregate(mean_travel_time ~ sourceid + centre, data = centre_cluster_change, function(x) max(x) - min(x))
  max_min_difference <- max_min_difference[max_min_difference$mean_travel_time >= 5*60,] # only keep time difference > 5 mins
  
  # save the results
  decreasing_cluster_diffmaxmin[[centre]] <- max_min_difference
}

# combine the results
decreasing_cluster_diffmaxmin <- do.call(rbind, decreasing_cluster_diffmaxmin)
# add zone name to the data based on zone id
decreasing_cluster_diffmaxmin <- merge(decreasing_cluster_diffmaxmin, s.geo[, 1:2], 
                                       by.x = 'sourceid', by.y = 'properties.MOVEMENT_ID')
for(centre in 2:length(centres)){
  decreasing_cluster_diffmaxmin[decreasing_cluster_diffmaxmin$centre == centre, 'dst'] <- centres[[centre]]
}
decreasing_cluster_diffmaxmin[decreasing_cluster_diffmaxmin$centre == 1, 'dst'] <- 'Manchester 060'

## flow plot

# data preparation

# add cooredinates info to both source and destination
flow_timedecrease <- merge(decreasing_cluster_diffmaxmin,
                           emp_flow_withcoord[, c('Zone', 'xcoord', 'ycoord')], by.x = 'properties.DISPLAY_NAME', by.y = 'Zone')
flow_timedecrease <- merge(flow_timedecrease,
                           emp_flow_withcoord[, c('Zone', 'xcoord', 'ycoord')],
                           by.x = 'dst', by.y = 'Zone')

flow_timedecrease <- flow_timedecrease[flow_timedecrease$properties.DISPLAY_NAME != flow_timedecrease$dst,] # remove flow of self-loop
flow_timedecrease <- flow_timedecrease %>% arrange(mean_travel_time)
flow_timedecrease$mean_travel_time <- flow_timedecrease$mean_travel_time/60 # convert from secs to mins

# focused flow data: travel times decrease >= 10 minutes
save10minutes <- flow_timedecrease[flow_timedecrease$mean_travel_time >= 10, ]

# plot flow map
ggplot() + geom_polygon(data = spdf_fortified, aes(x = long, y = lat, group = group), 
                        fill="grey", alpha = 0.3, color="white") + coord_fixed(1) + 
  geom_curve(data = save10minutes, curvature = 0.2,
             aes(x = xcoord.x, y = ycoord.x, xend = xcoord.y, yend = ycoord.y, 
                 color = mean_travel_time, size = mean_travel_time), arrow = arrow(length = unit(0.02, "npc"))) + 
  geom_label_repel(data = save10minutes[save10minutes$mean_travel_time >= 20,],
                   aes(x = xcoord.x, y = ycoord.x, label = properties.DISPLAY_NAME),
                   fill = c("orange"), color = "white",
                   point.padding = NA, box.padding = 0.1, vjust = 0.5) +
  geom_label_repel(data = unique(flow_timedecrease[, c('dst', 'xcoord.y', 'ycoord.y')]),
                   aes(x = xcoord.y, y = ycoord.y, label = c('Stockport', 
                                                             'Manchester city centre', 'Tameside', 
                                                             'Manchester Airport', 'Trafford')),
                   fill = alpha(c("white"), 0.8), label.size = NA, vjust = -1) +
  scale_size_continuous(name = "Travel time decrease (mins)", range=c(0.5, 2), breaks = c(10, 15, 20, 25), limits = c(10, 25)) + 
  scale_color_viridis(option="viridis", name = "Travel time decrease (mins)", breaks = c(10, 15, 20, 25), limits = c(10, 25)) + 
  scale_alpha_continuous(name = "Travel time decrease (mins)", range=c(0.1, .9), breaks = c(10, 15, 20, 25), limits = c(10, 25)) +
  theme_void() + guides(colour = guide_legend())

# ggsave('./images/travel_time_decrease.png')


### Find zones increasing the accessibility level the most

## Use time series clustering to find cluster with the target zones

# only leave data having travel times over half the period
length_ts_zone <- table(access_ts_30$sourceid)
access_ts_30 <- access_ts_30[access_ts_30$sourceid %in% names(which(length_ts_zone >= 15/2)),]
df_list <- as.list(utils::unstack(access_ts_30, centre_area ~ sourceid))
# data standardisation
df_list_z <- dtwclust::zscore(df_list)

# hierarchical clustering for up to k=10 clusters
num_k <- 10
cluster_dtw_h <- tsclust(df_list_z, type = "h", k = 2:num_k,  distance = "dtw", 
                         control = hierarchical_control(method = "complete"), seed = 518492, 
                         preproc = NULL)

# use CVIs to choose the optimal number of clusters
CVIs <- lapply(cluster_dtw_h, cvi, type = "internal")
CVIs <- data.frame(k = rep(c(1:(num_k-1)), each = length(CVIs[[1]])),
                   metrics = rep(names(CVIs[[1]]), num_k-1),
                   value = unlist(CVIs))
ggplot() + geom_line(data = CVIs, aes(x = k, y = value, group = metrics, color = metrics)) + 
  theme_light()

# time series cluster plot
# png('./images/access_cluster_sc.png', width=12,height=4,units='in', res=300)
plot(cluster_dtw_h[[1]], type = "sc")
# dev.off()

# the representative prototype 
# png('./images/access_cluster_centroid.png', width=12,height=4,units='in', res=300)
plot(cluster_dtw_h[[1]], type = "centroid")
# dev.off()

# the degree of accessibility level increase
max_min_difference <- aggregate(centre_area ~ sourceid, data = access_ts_30, 
                                function(x) max(x) - min(x))
# add zone name to the data
max_min_difference <- merge(max_min_difference, s.geo[, 1:2], by.x = 'sourceid', 
                            by.y = 'properties.MOVEMENT_ID')
# add coordinate info to the data
choropleth_accessincrease <- merge(max_min_difference,
                                   emp_flow_withcoord[, c('Zone', 'xcoord', 'ycoord')], 
                                   by.x = 'properties.DISPLAY_NAME', by.y = 'Zone')

# specify different colours to different increase in accessibility level
spdf_fortified_choropleth <- spdf_fortified
spdf_fortified_choropleth$colour <- 0
for(i in unique(max_min_difference$centre_area)){
  spdf_fortified_choropleth[spdf_fortified_choropleth$id %in% 
                              max_min_difference[max_min_difference$centre_area == i, 
                                                 'properties.DISPLAY_NAME'], 'colour'] <- i
}

# choropleth plot
ggplot() +
  geom_polygon(data = spdf_fortified_choropleth, aes(fill = colour, x = long, y = lat, group = group), 
               alpha = 0.3, color="grey") + 
  scale_fill_gradient(low="white", high="red", space ="Lab", name = 'Accessibility level increase') + 
  geom_label_repel(data = choropleth_accessincrease[choropleth_accessincrease$centre_area == 4,], 
                   aes(x = xcoord, y = ycoord, label = properties.DISPLAY_NAME), 
                   fill = c("#CC6666"), color = "white", segment.color = 'darkgrey',
                   point.padding = NA, box.padding = 0.1, hjust = 2, vjust = -0.3) + 
  geom_label_repel(data = unique(flow_timedecrease[, c('dst', 'xcoord.y', 'ycoord.y')]),
                   aes(x = xcoord.y, y = ycoord.y, label = c('Stockport', 
                                                             'Manchester city centre', 'Tameside', 
                                                             'Manchester Airport', 'Trafford')),
                   fill = alpha(c("white"), 0.8), label.size = NA, vjust = 1.5, hjust = -0.2) + 
  theme_void() + theme(legend.position = "top", text = element_text(size = 14)) + coord_fixed(1)

# ggsave('./images/accessibility_level_increase.png')
